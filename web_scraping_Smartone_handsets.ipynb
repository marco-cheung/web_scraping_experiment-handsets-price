{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with BeautifulSoup and Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "#### python 3.6\n",
    "#### chromedriver <-- a Chrome browser engine to initialize Chrome for automated running of Selenium-related script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prerequisites\n",
    "\n",
    "import requests  #for handling HTTP requests\n",
    "from bs4 import BeautifulSoup  #for parsing HTML\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait #for making WebDriver wait for an element to meet expected condition\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from openpyxl import load_workbook #for writing outout to excel file\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class urls_scraping():\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def fetch_urls(self):\n",
    "\n",
    "        #Simulate clicking 'Esc' to jump out pop-up window (if any)\n",
    "        webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()\n",
    "        \n",
    "        #tell Webdriver to access the website and scroll down in order to display all product urls on the page\n",
    "        def page_scroll_down():\n",
    "            driver.get(self.url)\n",
    "        \n",
    "            #Scroll down the page to the bottom\n",
    "            lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    lastCount = lenOfPage\n",
    "                    time.sleep(3)\n",
    "                    lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    \n",
    "                    view_button = driver.find_element_by_link_text(\"View More Products\")\n",
    "                    view_button.click()\n",
    "    \n",
    "                except NoSuchElementException:\n",
    "                    if lastCount==lenOfPage:\n",
    "                        #print (\"There's no more products at the bottom of this page.\")\n",
    "                        break\n",
    "        \n",
    "        #to scrap all products urls on the page\n",
    "        def urls_scrap():       \n",
    "            #Gather all products URLs on the web page at once\n",
    "            containers = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            global products_urls\n",
    "            products_urls.extend([base_url + link.a.get('href') for link in containers.findAll(\"div\", class_ = \"tile-inner\")])\n",
    "            products_urls = list(np.unique(products_urls))\n",
    "           \n",
    "        #execute inner functions when calling the outer function 'fetch_urls()'              \n",
    "        page_scroll_down()\n",
    "        urls_scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define website url which I am going to scrape\n",
    "url1 = \"https://shop.smartone.com/en/storefront/handset/listing/View-all-Smartphones/0/\"\n",
    "url2 = \"https://shop.smartone.com/en/storefront/iphone.jsp\"\n",
    "base_url = 'https://shop.smartone.com'\n",
    "products_urls = []\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #Create a new Chrome session\n",
    "    #define global variable so that we can access that variable outside the scope of the function\n",
    "    global driver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(5) #tell WebDriver to elapse 5 seconds when trying to find any element (or elements) not immediately available   \n",
    "    \n",
    "    get_urls1 = urls_scraping(url1)\n",
    "    get_urls1.fetch_urls()\n",
    "    \n",
    "    get_urls2 = urls_scraping(url2)\n",
    "    get_urls2.fetch_urls()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finally fetched 37 products URLs.\n"
     ]
    }
   ],
   "source": [
    "#remove the non-product url\n",
    "products_urls = [x for x in products_urls if x.startswith('https://shop.smartone.com/en/storefront/mobile')]\n",
    "                 \n",
    "print (\"We finally fetched \" + str(len(products_urls)) + \" products URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define field of product details\n",
    "brand = []\n",
    "product_name = []\n",
    "color = [] #different colour options may result in different prices\n",
    "capacity_spec = [] #different capacity options may result in different prices\n",
    "RRP = []   #recommended retail price\n",
    "selling_price = []\n",
    "stock_status = []\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is completed.\n",
      "Wall time: 22min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in products_urls:\n",
    "    driver.get(i)\n",
    "    \n",
    "    #find capacity option(s)\n",
    "    colour = driver.find_elements_by_xpath('//div[contains(@class, \"st-color-chooser\")]/a[contains(@class, \"color-swatch\")]')\n",
    "    \n",
    "    for x in colour:\n",
    "        driver.execute_script(\"arguments[0].click()\", x)\n",
    "        \n",
    "        #find capacity option(s)\n",
    "        capacity = driver.find_elements_by_xpath('//span[contains(@class, \"st-model-wrapper st-box-shadow\")]/a[starts-with(@id,\"model\")]')\n",
    "        \n",
    "        for elem in capacity:\n",
    "            driver.execute_script(\"arguments[0].click()\", elem)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            #Use driver.page_source to get the HTML as it appears after javascript has rendered it\n",
    "            page_source = driver.page_source\n",
    "                \n",
    "            #Use a parser on the returned HTML\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            time.sleep(3)\n",
    "                \n",
    "            #scrap capacity\n",
    "            capacity_spec.append(elem.get_attribute('title'))\n",
    "            \n",
    "            #scrap brand\n",
    "            brand.append(soup.find(\"h3\", itemprop = \"brand\").text.upper())\n",
    "                \n",
    "            #scrap product name\n",
    "            product_name.append(soup.find(\"h1\", itemprop = \"name\").text.upper())\n",
    "            \n",
    "            #scrap colour\n",
    "            color.append(x.get_attribute('title'))\n",
    "                \n",
    "            #refresh the parser\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            #scrap stock status\n",
    "            status = soup.find('div', class_ = re.compile('^st-stock-status .*'))\n",
    "            if status.get_text() != '':\n",
    "                stock_status.append(status.get_text())\n",
    "            else:\n",
    "                stock_status.append('in stock')\n",
    "                \n",
    "            #scrap RRP\n",
    "            RRP0 = soup.find(\"span\", class_ = \"st-ori-price\").text.replace('HK$','').replace(',','')\n",
    "            if RRP0 == '':\n",
    "                RRP.append(soup.find(\"span\", class_ = \"st-price st-hs-view-only\").text.replace('HK$','').replace(',',''))\n",
    "            else:\n",
    "                RRP.append(RRP0)\n",
    "                \n",
    "            #scrap selling (final) price\n",
    "            selling_price.append(soup.find(\"span\", class_ = \"st-price st-hs-view-only\").text.replace('HK$','').replace(',',''))\n",
    "            \n",
    "            #scrap product_url\n",
    "            product_url.append(i)\n",
    "\n",
    "driver.close()\n",
    "print (\"Scraping is completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To zip 8 lists together and then construct dataframe\n",
    "list_of_tuples = list(zip(brand, product_name, color, capacity_spec, RRP, selling_price, stock_status, product_url))\n",
    "df = pd.DataFrame(list_of_tuples, columns = ['Brand', 'Product Name', 'Colour', 'Capacity', 'RRP', 'Selling Price', 'Stock Status', 'Product URL'])\n",
    "\n",
    "#Indicate \"Fortress\" as data source\n",
    "df.insert(0, 'Source', 'SmarTone')\n",
    "\n",
    "#Populate current date in new column for visualizing time-series data\n",
    "df['Date'] = pd.to_datetime('today').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert all characters to upper case for Product Name\n",
    "#Trim all spaces from the text string except for single spaces between words\n",
    "df['Product Name'] = df['Product Name'].str.replace(' RAM','').str.strip()\n",
    "df['Product Name'] = df['Product Name'].str.replace('GALAXY','').str.strip() \n",
    "df['Product Name'] = df['Product Name'].str.replace('NOTE','NOTE ').str.strip()\n",
    "\n",
    "#Remove year (e.g. 2018, 2019)\n",
    "year = ['2017','2018','2019','2020','2021','2022','2023','2024']\n",
    "df['Product Name'] = df['Product Name'].str.replace('|'.join([re.escape(s) for s in year]), '')\n",
    "df['Product Name'] = df['Product Name'].str.strip()\n",
    "\n",
    "#To convert all characters to upper case and remove all special characters, parentheses and stuff within for 'Colour'\n",
    "df['Colour'] = df['Colour'].str.upper()\n",
    "df['Colour'] = [re.sub('[^A-Z0-9 \\n]', '', x) for x in df['Colour']]\n",
    "df['Colour'] = df['Colour'].str.strip()\n",
    "\n",
    "#To remove all whitespace and add parenthesis for 'Capacity'\n",
    "df['Capacity'] = df['Capacity'].str.replace(' ','').str.strip()\n",
    "df['Capacity'] = [''.join('(' + item + ')').replace('()','') for item in df['Capacity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brand XIAOMI => MI\n",
    "df['Brand'] = df['Brand'].str.replace('XIAOMI','MI')\n",
    "\n",
    "#Missing (brand) data imputation\n",
    "#Fill in missing Brand Name 'Apple' if the products are IPHONE (p.s. since brand is not found in IPHONE landing product page)\n",
    "df.loc[df['Product Name'].str.contains('IPHONE'), 'Brand'] = 'APPLE'\n",
    "\n",
    "#Get list of unique brand\n",
    "unique_brand = df['Brand'].unique().tolist()\n",
    "\n",
    "#Problem found: in some cases, product name does not include brand (e.g.'GALAXY NOTE9')\n",
    "#Solution: Align product name display (i.e. brand + name)\n",
    "#1. remove brand name in product name first\n",
    "p = re.compile('|'.join(map(re.escape, unique_brand)))\n",
    "df['Product Name'] = [p.sub('', text).strip() for text in df['Product Name']]\n",
    "\n",
    "#2. concatenate Brand Name and product name to align product name format\n",
    "df['Product Name'] = df['Brand'].map(str) + ' ' + df['Product Name'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, rename convention of Product Name = Product Name + Capacity\n",
    "for i in df['Product Name']:\n",
    "    df['Name'] = df['Product Name'].str.cat(df['Capacity'],sep=\" \").str.strip()\n",
    "\n",
    "df.drop('Product Name', axis=1, inplace=True)\n",
    "df = df.rename({'Name': 'Product Name'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace multi-spacing between with single whitespace within product name\n",
    "df['Product Name'] = [' '.join(str1.split()) for str1 in df['Product Name']]\n",
    "\n",
    "#Create dictionary for special handling of product name (to align naming across other sources)\n",
    "dict1 = {\n",
    "    \"NOKIA 7.2 (128GB)\": \"NOKIA 7.2\",\n",
    "    \"SAMSUNG A70 (6GB) (128GB)\": \"SAMSUNG A70 (6GB)\",\n",
    "    \"SAMSUNG A70 (8GB) (128GB)\": \"SAMSUNG A70 (8GB)\"\n",
    "}\n",
    "\n",
    "#Remap the values of the dataframe \n",
    "df.replace({'Product Name': dict1}, inplace=True)\n",
    "\n",
    "#Brand 'SONY XPERIA' => 'SONY'\n",
    "df['Brand'] = df['Brand'].str.replace('SONY XPERIA','SONY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalize dataframe column order\n",
    "df = df[['Date', 'Source', 'Brand', 'Product Name', 'Colour', 'Capacity', 'RRP', 'Selling Price', 'Stock Status', 'Product URL']]\n",
    "\n",
    "df1 = df[['Date', 'Source', 'Brand', 'Product Name', 'RRP', 'Selling Price']]\n",
    "\n",
    "#Drop duplicates product and keep='first' to keep first of duplicates\n",
    "#In case one product is selling at different prices for different colours, then only keep first of duplicates for each product name\n",
    "#(i.e. given that most products are selling the same price for different colours)\n",
    "cleaned_df1 = df1.drop_duplicates(subset=['Product Name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To write dataframe to new Excel file and append data to 'Aggregated_SmarTone_handsets_offer.csv'\n",
    "CurrentDate = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "root = 'SmarTone'\n",
    "\n",
    "df.to_excel(root + '/' + CurrentDate + ' SmarTone handsets offer' + '.xlsx', sheet_name = 'SmarTone_handsets', index = False, encoding='utf-8-sig')\n",
    "\n",
    "#Stack the DataFrames on top of 'Aggregated_Fortress_headsets_offer.csv'\n",
    "#cleaned_df1.to_csv('Aggregated_SmarTone_headsets_offer.csv', index = False, header = False, mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
