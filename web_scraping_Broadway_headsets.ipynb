{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with BeautifulSoup and Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "#### python 3.6\n",
    "#### chromedriver <-- a Chrome browser engine to initialize Chrome for automated running of Selenium-related script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prerequisites\n",
    "\n",
    "import requests  #for handling HTTP requests\n",
    "from bs4 import BeautifulSoup  #for parsing HTML\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait #for making WebDriver wait for an element to meet expected condition\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options #Set options for running headless Chrome browser with Selenium\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define website url which I am going to scrape\n",
    "url = \"https://www.broadwaylifestyle.com/categories/usage/mobile-products/mobile-phone.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new Chrome session\n",
    "options = Options()\n",
    "options.headless = True #to initiate chrome browser in headless mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)\n",
    "#tell WebDriver to elapse 5 seconds when trying to find any element (or elements) not immediately available\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "#Simulate clicking 'Esc' to jump out pop-up window (if any)\n",
    "webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page to the bottom\n",
    "lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "\n",
    "while(match==False):\n",
    "    lastCount = lenOfPage\n",
    "    time.sleep(3)\n",
    "    lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    \n",
    "    if lastCount==lenOfPage:\n",
    "        match=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change webpage display in English\n",
    "footer_button = driver.find_element_by_class_name('footer_title')\n",
    "footer_button.click()#clicklink\n",
    "\n",
    "try:\n",
    "    eng_ver = driver.find_element_by_link_text(\"ENG\")\n",
    "    eng_ver.click()\n",
    "except NoSuchElementException:\n",
    "    print(\"English version already.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no more products at the bottom of this page.\n"
     ]
    }
   ],
   "source": [
    "#Scroll down the page to the bottom\n",
    "lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        lastCount = lenOfPage\n",
    "        lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    \n",
    "        time.sleep(3)\n",
    "        view_button = driver.find_element_by_link_text(\"View More Products\")\n",
    "        view_button.click()\n",
    "    \n",
    "    except NoSuchElementException:\n",
    "        if lastCount==lenOfPage:\n",
    "            print (\"There's no more products at the bottom of this page.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finally fetched 85 products URLs.\n"
     ]
    }
   ],
   "source": [
    "containers = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "products_urls = [link.a.get('href') for link in containers.findAll(\"div\", class_ = \"product-item-info\")]\n",
    "\n",
    "products_urls = list(np.unique(products_urls))\n",
    "\n",
    "print(\"We finally fetched \" + str(len(products_urls)) + \" products URLs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to build and maintain an efficient automated web scraper, firstly we need to understand the structure of the website, e.g. html source code, XPath, etc.\n",
    "#### (p.s. click F12 to inspect the element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle url randomly to ensure browser can access the url followed by same product with different color options\n",
    "random.shuffle(products_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define field of product details\n",
    "brand = []\n",
    "product_name = []\n",
    "color = [] #different colour options may result in different prices\n",
    "capacity_spec = [] #different capacity options may result in different prices\n",
    "RRP = []   #recommended retail price\n",
    "final_price = []\n",
    "free_gift = []\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping is completed.\n",
      "Wall time: 22min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in products_urls:\n",
    "    driver.get(i)\n",
    "       \n",
    "    driver.refresh()\n",
    "    \n",
    "    #Give the javascript time to render\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Use driver.page_source to get the HTML as it appears after javascript has rendered it\n",
    "    page_source = driver.page_source\n",
    "              \n",
    "    #Use a parser on the returned HTML\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    \n",
    "    try:      \n",
    "        view_more_button = driver.find_element_by_link_text(\"Click to Read More\")\n",
    "        view_more_button.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #for x in colour:\n",
    "        #driver.execute_script(\"arguments[0].click()\", x)\n",
    "             \n",
    "        #scrap brand\n",
    "        brand.append(soup.find(\"div\", class_ = \"brand-name\").text)\n",
    "                               \n",
    "        #scrap product name\n",
    "        product_name.append(soup.find(\"span\", itemprop = \"name\").text.upper())\n",
    "            \n",
    "        #scrap capacity\n",
    "        capacity_spec.append(soup.find(attrs={\"data-th\" : 'Rom (GB)'}).text)\n",
    "                            \n",
    "        #scrap colour\n",
    "        #colour = driver.find_elements_by_xpath('//div[contains(@class, \"swatch-option\")][not(contains(@class, \"swatch-option disabled\"))]')\n",
    "        \n",
    "        colour = driver.find_element_by_xpath('//div[contains(@class, \"selected\")]')\n",
    "        color.append(colour.get_attribute('option-label'))\n",
    "                \n",
    "        #scrap RRP\n",
    "        if soup.find(\"span\", class_ = \"old-price\") is not None:\n",
    "            RRP.append(soup.find(\"span\", class_ = \"price-inner\").text.replace('HK$ ','').replace(',',''))\n",
    "        else:\n",
    "            RRP.append('')\n",
    "                \n",
    "        #scrap final price\n",
    "        final_price.append(soup.find(\"span\", class_ = \"price\").text.replace('HK$ ','').replace(',',''))\n",
    "                        \n",
    "        #scrap product_url\n",
    "        product_url.append(i)\n",
    "        \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "driver.close()\n",
    "print (\"Scraping is completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To zip 8 lists together and then construct dataframe\n",
    "list_of_tuples = list(zip(brand, product_name, color, capacity_spec, RRP, final_price, product_url))\n",
    "df = pd.DataFrame(list_of_tuples, columns = ['Brand', 'Product Name', 'Colour', 'Capacity', 'RRP', 'Selling Price', 'Product URL'])\n",
    "\n",
    "#sort dataframe by Product Name ('A-Z')\n",
    "df = df.sort_values('Product Name')\n",
    "\n",
    "#Indicate \"Broadway\" as data source\n",
    "df.insert(0, 'Source', 'Broadway')\n",
    "\n",
    "#Populate current date in new column for visualizing time-series data\n",
    "df['Date'] = pd.to_datetime('today').date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To convert all characters to upper case and remove all parentheses and stuff within for Product Name\n",
    "#Example: 'HUAWEI MATE20 PRO (6GB/128GB) (BK)' => 'HUAWEI MATE20 PRO'\n",
    "df['Product Name'] = df['Product Name'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "\n",
    "#To remove word 'SMARTPHONE'/'VERSION'/'MOBILE PHONE' from product name and strip space from the beginning and the end of the string \n",
    "df['Product Name'] = df['Product Name'].str.replace('SMARTPHONE','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('VERSION','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('GAMING','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('CRYSTAL-WH/BL','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('SUNRISE-OR','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('FLIP  TA-1170','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('MOBILE PHONE','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('GALAXY','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('MATE20','MATE 20')\n",
    "df['Product Name'] = df['Product Name'].str.replace('MATE 20 X','MATE 20X')\n",
    "df['Product Name'] = df['Product Name'].str.replace('NOTE','NOTE ')\n",
    "df['Product Name'] = df['Product Name'].str.replace('TA-1164 DS 3/32 HK','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('TA-1196 DS 6/128 HK','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('TA-1189','')\n",
    "df['Product Name'] = df['Product Name'].str.replace('TA-1170','')\n",
    "\n",
    "#Remove year (e.g. 2018, 2019)\n",
    "year = ['2017','2018','2019','2020','2021','2022','2023','2024']\n",
    "df['Product Name'] = df['Product Name'].str.replace('|'.join([re.escape(s) for s in year]), '')\n",
    "\n",
    "#Trim all spaces from the text string except for single spaces between words\n",
    "df['Product Name'] = df['Product Name'].str.strip()\n",
    "\n",
    "#To convert all characters to upper case and remove all special characters, parentheses and stuff within for Colour\n",
    "df['Colour'] = df['Colour'].str.upper()\n",
    "df['Colour'] = [re.sub('[^A-Z0-9 \\n]', '', x) for x in df['Colour']]\n",
    "df['Colour'] = df['Colour'].str.strip()\n",
    "\n",
    "#To remove all whitespace and add parenthesis for 'Capacity'\n",
    "df['Capacity'] = df['Capacity'].str.replace(' ','')\n",
    "df['Capacity'] = [''.join('(' + item + ')').replace('()','') for item in df['Capacity']]\n",
    "\n",
    "#To remove blank line for 'RRP' and 'Selling Price'\n",
    "df['RRP'] = df['RRP'].str.replace('\\n','').str.strip()\n",
    "df['Selling Price'] = df['Selling Price'].str.replace('\\n','').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of unique brand\n",
    "unique_brand = df['Brand'].unique().tolist()\n",
    "\n",
    "#Problem found: in some cases, product name does not include brand (e.g.'GALAXY A8+')\n",
    "#Solution: Align product name display (i.e. brand + name + capacity)\n",
    "#1. remove brand name in product name first\n",
    "p = re.compile('|'.join(map(re.escape, unique_brand)))\n",
    "df['Product Name'] = [p.sub('', text).strip() for text in df['Product Name']]\n",
    "\n",
    "#2. concatenate Brand Name and product name to align product name format\n",
    "df['Product Name'] = df['Brand'].map(str) + ' ' + df['Product Name'].map(str) + ' ' + df['Capacity'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary for special handling of product name\n",
    "dict1 = {\n",
    "    \"SAMSUNG A70 (128GB)\": \"SAMSUNG A70 (6GB)\", #wrongly marked by Broadway\n",
    "    \"NOKIA 7.2 (128GB)\": \"NOKIA 7.2\"\n",
    "}\n",
    "\n",
    "# Remap the values of the dataframe \n",
    "df.replace({'Product Name': dict1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Date column to datetime\n",
    "df['Date'] = df['Date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "#Finalize dataframe column order\n",
    "df = df[['Date', 'Source', 'Brand', 'Product Name', 'Colour', 'Capacity', 'RRP', 'Selling Price', 'Product URL']]\n",
    "\n",
    "df1 = df[['Date', 'Source', 'Brand', 'Product Name', 'RRP', 'Selling Price']]\n",
    "\n",
    "#Drop duplicates product and keep='first' to keep first of duplicates\n",
    "#In case one product is selling at different prices for different colours, then only keep first of duplicates for each product name\n",
    "#(i.e. given that most products are selling the same price for different colours)\n",
    "cleaned_df1 = df1.drop_duplicates(subset=['Product Name'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write dataframe to new Excel file and append data to 'Aggregated_Broadway_headsets_offer.csv'\n",
    "CurrentDate = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "root = 'Broadway'\n",
    "\n",
    "df.to_excel(root + '/' + CurrentDate + ' Broadway headsets offer' + '.xlsx', sheet_name = 'Broadway_headsets', index = False, encoding='utf-8-sig')\n",
    "\n",
    "# Stack the DataFrames on top of 'Aggregated_Broadway_headsets_offer.csv'\n",
    "cleaned_df1.to_csv('Aggregated_Broadway_headsets_offer.csv', index = False, header = False, mode='a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
